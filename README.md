# Quote_scraping

A small collection of example web-scraping projects written in Python: one using Scrapy (a full crawler) and another using Requests + BeautifulSoup (a simple scraper). Both projects target sites intentionally provided for scraping practice.

This README explains the repository structure, the purpose of each file, installation instructions, how to run the scrapers, expected output files, and a short note about ethics and licensing.

---

Table of contents
- Overview
- Repository structure (file descriptions)
- Setup & dependencies
- Running the projects
  - Quotes Scraper (Scrapy)
  - Books Scraper (BeautifulSoup)
- Output files and example results
- Notes on scraping ethics
- Contributing
- License

---

Overview
--------
This repository demonstrates two different approaches to web scraping:

- Quotes Scraper (Scrapy): a spider that crawls the "Quotes to Scrape" demo site, follows pagination and author pages, and exports structured data (quotes, authors, tags, author biography details).
- Books Scraper (BeautifulSoup): a script using Requests + BeautifulSoup to crawl "Books to Scrape", handling pagination and categories, collecting book metadata, and downloading cover images.

Both targets are public practice sites and safe to scrape.

Repository structure (what each file/folder is for)
--------------------------------------------------
Root files
- README.md
  - This file: overview, usage, and file descriptions.
- scrape.py
  - Main BeautifulSoup-based book scraping script. Crawls categories/pagination, extracts book data, downloads cover images, and writes a CSV.
- books_full_scrape.csv
  - Example/generated output from running `scrape.py`. Contains the aggregated book dataset (title, price, availability, description, rating, image URL, category, etc.).
- images/
  - Folder produced by `scrape.py` containing downloaded book cover images organized by category.
  - Example subfolders: `Travel/`, `Mystery/`, etc.

Scrapy project
- quotes_scraper/
  - A standard Scrapy project directory.
  - scrapy.cfg
    - Scrapy configuration file for the project.
  - quotes_scraper/
    - spiders/
      - quotes_spider.py
        - The Scrapy spider that crawls the Quotes to Scrape site, follows pagination and author pages, and yields items.
    - items.py
      - Scrapy item definitions (fields collected by the spider).
    - settings.py
      - Scrapy settings for the project (concurrency, pipelines, user-agent, etc.).
    - (other Scrapy files as generated by `scrapy startproject`)

Setup & dependencies
--------------------
General
- Python 3.8+ recommended.
- Use a virtual environment for isolation (venv, virtualenv, conda, etc.).

Books Scraper (BeautifulSoup)
- Install dependencies:
  - pip install requests beautifulsoup4 lxml
  - (Optional) pip install pandas if you want to manipulate CSV outputs in Python.
- The script relies on Requests for HTTP and BeautifulSoup for parsing.

Quotes Scraper (Scrapy)
- Install Scrapy:
  - pip install scrapy
- The Scrapy project uses the Scrapy command-line API to run the spider.

Running the projects
--------------------

Quotes Scraper (Scrapy)
1. Change into the Scrapy project folder (if needed):
   - cd quotes_scraper
2. Run the spider and export to JSON (or CSV):
   - scrapy crawl quotes_spider -o quotes.json
   - Or export to CSV: scrapy crawl quotes_spider -o quotes.csv
3. Result:
   - `quotes.json` (or `quotes.csv`) will contain one entry per quote with author details included (or references to author pages depending on the spider implementation).

Notes:
- You can change export format by changing the output filename extension to any format supported by Scrapy (jsonlines, json, csv, xml).
- If you want to run Scrapy from the repository root, ensure you reference the project correctly or run via `python -m scrapy` from inside the `quotes_scraper` project.

Books Scraper (BeautifulSoup)
1. Ensure dependencies are installed:
   - pip install requests beautifulsoup4 lxml
2. Run the script:
   - python scrape.py
3. Result:
   - `books_full_scrape.csv` — CSV file with all scraped book metadata.
   - `images/` — folder containing downloaded book cover images organized by category.

Script behavior highlights
- Pagination handling: both scrapers follow pagination to iterate all pages.
- Category crawling: the Books scraper iterates all categories to gather complete dataset.
- Image download: `scrape.py` downloads book cover images and saves them in `images/<Category>/`.
- Export: output CSV/JSON contains all captured fields: titles, prices, availability, descriptions, ratings, image URLs, categories, and for the quotes scraper — author biography details.

Output files and examples
-------------------------
- quotes.json (generated): example structure
  - quote_text
  - author_name
  - tags
  - author_page_url
  - author_born_date
  - author_born_location
  - author_description

- books_full_scrape.csv (generated): columns typically include
  - title, price, availability, product_description, star_rating, image_url, category, image_local_path

Notes on scraping ethics and legality
------------------------------------
- The projects target demo websites intentionally provided for scraping practice:
  - http://quotes.toscrape.com/
  - http://books.toscrape.com/
- Always:
  - Check a site's robots.txt and terms of service before scraping non-demo sites.
  - Rate-limit requests and avoid high concurrency on production sites.
  - Identify your scraper using a descriptive User-Agent if required.
  - Respect data privacy and copyright.

Contributing
------------
- Bug reports, improvements, and PRs are welcome.
- When contributing:
  - Open an issue describing the change or bug.
  - Create a branch, make changes, and open a PR referencing the issue.
  - Keep code style consistent and include tests or example outputs for substantial changes.

License
-------
- Add your chosen license here. If there is no license file, the repository defaults to “All rights reserved” (no permission to reuse). Consider adding an OSI-approved license such as MIT, Apache-2.0, or similar.

Contact
-------
- For questions, feature requests, or help: open an issue on the repository.

---
